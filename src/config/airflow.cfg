[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /usr/local/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /usr/local/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# The class to use for running task instances in parallel.
# Choices include SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool.
sql_alchemy_pool_size = 5

# The file to store the pid while running the deamon
pid = /usr/local/airflow/airflow-webserver.pid

# The default timezone to display in the UI
default_timezone = utc

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is necessary for many features including
# sending emails.
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is disabled. When nonzero,
# airflow periodically refreshes webserver workers by bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 30

# Secret key used to run your flask app
secret_key = temporary_key

# Number of threads to run the Gunicorn web server
workers = 4

[scheduler]
# Task instances are run by the executor, see the section on Executors for more information.
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5

# Number of seconds to wait before checking on the status of the job.
scheduler_idle_sleep_time = 1

# Scheduler will limit the number of task instances that it will run
# in parallel to this number
max_threads = 2

# The number of task instances allowed to run concurrently by the scheduler
max_active_tasks_per_dag = 16

# If a task instance is in the UP_FOR_RETRY state, the scheduler will
# wait this many seconds between retries.
retry_delay = 300

# Time to wait before a DAG can be run after the start date
min_file_process_interval = 0

# Directory to search for DAG definitions
dags_folder = /usr/local/airflow/dags

# How often (in seconds) to check for new DAGs
dag_dir_list_interval = 300

# If set, the scheduler will be started on the Web server's entrypoint.
run_scheduler = True

[logging]
# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Path to the remote log base. This will be appended to the base_log_folder to create the remote
# log path. If using S3 or GCS, you need to specify the full URL to the remote log folder.
remote_base_log_folder =

# The URL for ElasticSearch. Set this to connect to ElasticSearch and use it for log storage.
elasticseach_endpoint =

# When logs are already in remote storage, prevent Airflow from writing the logs locally
disable_local_log_storage = False

# Set the remote log level (useful when the default log level is too verbose)
remote_log_level = INFO

# When using s3, setting this to False will write logs to s3 without using multipart upload
use_webserver_s3_url = False

[smtp]
# If you want airflow to send emails on retries, failure, and you want to the
# ability to email task logs, you need to configure an smtp server here
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
# Example: smtp_user = airflow
smtp_user = YOUR_SMTP_USER
# Example: smtp_password = airflow
smtp_password = YOUR_SMTP_PASSWORD
smtp_port = 587
smtp_mail_from = airflow@example.com

[celery]
# This section only applies if you are using the CeleryExecutor in [core] section above
celery_app_name = airflow.executors.celery_executor
worker_concurrency = 16

[celery_broker_transport_options]
visibility_timeout = 21600

[dask]
# This section only applies if you are using the DaskExecutor in [core] section above
cluster_address = 127.0.0.1:8786
tls_ca = None
tls_cert = None
tls_key = None

[metrics]
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[ldap]
# set this to True if you want to enable LDAP authentication
# use_ldap = False
