version: "3"
services:

    airflow-webserver:
        hostname: airflow
        container_name: airflow
        image: andrejunior/airflow-spark:latest
        restart: always
        networks:
            - airflow
        depends_on:
            - postgres
            - minio
            - spark-master
            - spark-worker
            - hadoop-namenode
            - hadoop-datanode
        environment:   
            - AIRFLOW__CORE__LOAD_EXAMPLES=False
            - LOAD_EX=n
            - EXECUTOR=Local    
        volumes:
            - airflow-data:/usr/local/airflow/data
            - ./src/dags:/usr/local/airflow/dags
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets     
        ports:
            - "8085:8080"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    postgres:
        hostname: postgres
        container_name: postgres
        image: 'postgres:14-bullseye'
        environment:
            POSTGRES_USER: 'airflow'
            POSTGRES_PASSWORD: 'airflow'
            POSTGRES_DB: 'airflow'
            PGDATA: /data/postgres
        volumes:
            - postgres:/data/postgres
        ports:
            - "5432:5432"
        networks:
            - airflow
        restart: on-failure
        healthcheck:
            test: ["CMD", "pg_isready"]
            interval: 60s
            timeout: 20s
            retries: 3
        deploy:
          resources:
            limits:
              memory: 400MB

    minio:
        hostname: bucket 
        container_name: bucket
        image: 'bitnami/minio:latest'
        environment:
            MINIO_ROOT_USER: airflow
            MINIO_ROOT_PASSWORD: airflow
        ports:
            - '9000:9000'
            - '9001:9001'
        volumes:
            - minio_data:/data
        networks:
            - airflow
        healthcheck:
            test: ["CMD", "curl", "-f", "http://bucket:9000/minio/health/live"]
            interval: 60s
            timeout: 20s
            retries: 3
        deploy:
          resources:
            limits:
              memory: 400MB

    createbuckets:
        image: minio/mc
        networks:
            - airflow
        depends_on:
            - minio
        entrypoint: >
            /bin/sh -c "
            /usr/bin/mc config host add myminio http://bucket:9000 airflow airflow;
            /usr/bin/mc rm -r --force myminio/airflow;
            /usr/bin/mc mb myminio/airflow;
            /usr/bin/mc policy download myminio/airflow;
            exit 0;
            "
            
    spark-master:        
        image: bitnami/spark:3.2.1
        # https://docs.bitnami.com/tutorials/work-with-non-root-containers/
        user: root 
        hostname: spark
        container_name: spark
        networks:
            - airflow
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets 
        ports:
            - "8081:8080"
            - "7077:7077"
        deploy:
          resources:
            limits:
              memory: 500MB

    spark-worker:
        image: bitnami/spark:3.2.1
        user: root
        hostname: spark-worker
        container_name: spark-worker
        networks:
            - airflow
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ./src/spark/applications:/usr/local/spark/applications            
            - ./src/spark/assets:/usr/local/spark/assets 
        depends_on:
            - spark-master
        deploy:
          resources:
            limits:
              memory: 1GB

    hadoop-namenode:
        image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
        container_name: hadoop-namenode
        networks:
            - airflow
        environment:
            - CLUSTER_NAME=test
            - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
        volumes:
            - hadoop-namenode:/hadoop/dfs/name
        ports:
            - "9870:9870"
            - "8020:8020"

    hadoop-datanode:
        image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
        container_name: hadoop-datanode
        networks:
            - airflow
        environment:
            - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
            - CLUSTER_NAME=test
        volumes:
            - hadoop-datanode:/hadoop/dfs/data
        ports:
            - "9864:9864"
        depends_on:
            - hadoop-namenode

volumes:
    postgres:
    airflow-data:
    minio_data:
    hadoop-namenode:
    hadoop-datanode:

networks:
    airflow:
        driver: bridge
