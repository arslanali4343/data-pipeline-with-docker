CORE-SITE.XML_fs.default.name=hdfs://namenode
CORE-SITE.XML_fs.defaultFS=hdfs://namenode
HDFS-SITE.XML_dfs.namenode.rpc-address=namenode:8020
HDFS-SITE.XML_dfs.replication=1
MAPRED-SITE.XML_mapreduce.framework.name=yarn
MAPRED-SITE.XML_yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=$HADOOP_HOME
MAPRED-SITE.XML_mapreduce.map.env=HADOOP_MAPRED_HOME=$HADOOP_HOME
MAPRED-SITE.XML_mapreduce.reduce.env=HADOOP_MAPRED_HOME=$HADOOP_HOME
YARN-SITE.XML_yarn.resourcemanager.hostname=resourcemanager
YARN-SITE.XML_yarn.nodemanager.pmem-check-enabled=false
YARN-SITE.XML_yarn.nodemanager.delete.debug-delay-sec=600
YARN-SITE.XML_yarn.nodemanager.vmem-check-enabled=false
YARN-SITE.XML_yarn.nodemanager.aux-services=mapreduce_shuffle
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-applications=10000
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-am-resource-percent=0.1
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.queues=default
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.capacity=100
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.user-limit-factor=1
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.maximum-capacity=100
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.state=RUNNING
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_submit_applications=*
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_administer_queue=*
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.node-locality-delay=40
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings=
CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings-override.enable=false
[logging]
# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Users must supply an Airflow connection id that provides access to the storage
# location.
remote_log_conn_id =
# Path to the remote log folder
# For S3: s3://my-bucket/path/to/logs
# For GCS: gs://my-bucket/path/to/logs
# For elastic search: use relative path from the urlbase, e.g. /elasticsearch
remote_base_log_folder =

# Use server-side encryption for logs stored in S3
encrypt_s3_logs = False

# Logging level
logging_level = INFO

# Logging class
logging_config_class = airflow.utils.log.logging_mixin.DEFAULT_LOGGING_CONFIG

# Log format for when a task is run
task_log_format = [%%(asctime)s] {%%(process)d:%%(processName)s} {%%(threadName)s} %%(
filename)s:%%(lineno)d - %%(levelname)s - %%(message)s

# Log format for when a task is run
log_format = [%%(asctime)s] {%%(process)d:%%(processName)s} {%%(threadName)s} %%(
filename)s:%%(lineno)d - %%(levelname)s - %%(message)s

# Specify the integration method for logging to stdout
# Valid values are "standard" and "json"
colored_console_log = True

colored_log_format = [%%(asctime)s] {%%(process)d:%%(processName)s} {%%(threadName)s} %%(
filename)s:%%(lineno)d - %%(levelname)s - %%(message)s

colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

# Specify which kind of task instance log should be sent to stdout
# Options are: `stream`, `console` or `both`
worker_log_server = stream
